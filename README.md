# ğŸ§  Mnemo

LangChain-powered Universal Memory System for AI Assistants  
Advanced memory orchestration with optional LangGraph workflows.

A modern memory system built on LangChain with ChromaDB, designed to work seamlessly with AI assistants like Cursor. Features sophisticated memory management, RAG capabilities, and optional advanced workflows with LangGraph.

## ğŸš€ Features

- **ğŸ”— LangChain-Powered**: Built on LangChain for seamless integration
- **ğŸ§  Vector Memory**: ChromaDB-based semantic memory with embeddings
- **ğŸ¤– Flexible Embeddings**: Choose from multiple embedding models:
  - Qwen3-Embedding-0.6B (lightweight, M3 optimized)
  - E5-Large (high quality, multilingual)
  - OpenAI (if API key available)
- **â›“ï¸ Memory Chains**: Pre-built LangChain chains for memory-enhanced conversations
- **ğŸ› ï¸ LangChain Tools**: Memory tools for agents and complex workflows
- **ğŸ“Š LangGraph Support**: Optional advanced workflows for complex scenarios
- **ğŸ¯ RAG Capabilities**: Retrieval-Augmented Generation with memory
- **ğŸ·ï¸ Rich Metadata**: Type-safe memory with tags, scopes, and priorities
- **ğŸ”Œ MCP Compatible**: Full Model Context Protocol integration for Cursor

## ğŸƒ Quick Start

### Installation

```bash
# Create conda environment
conda create -n mnemo python=3.11 -y
conda activate mnemo

# Basic installation
pip install -e .

# With optional dependencies
pip install -e ".[graph]"  # For LangGraph workflows
pip install -e ".[all]"    # Everything included

# For MCP support with Cursor
pip install -e ".[mcp]"
```

### Basic Usage

```python
from mnemo.memory.store import MnemoVectorStore
from mnemo.memory.client import MnemoMemoryClient

# Create memory system
vector_store = MnemoVectorStore(
    collection_name="my_memories",
    persist_directory="./memory_db"
)
client = MnemoMemoryClient(vector_store)

# Set context
client.set_context(workspace_path="/my/project")

# Remember something
memory_id = client.remember_fact(
    key="project_goal",
    fact="Build an awesome AI assistant with LangChain",
    tags={"project", "goals", "langchain"}
)

# Search memories (semantic search with embeddings)
results = client.search("AI assistant", limit=5)
print(f"Found {len(results)} related memories")

# Recall most relevant
content = client.recall("What is the project goal?")
print(f"Goal: {content}")
```

### CLI Usage

```bash
# Remember a fact
mnemo remember "project_structure" "LangChain app with FastAPI backend" --memory-type "fact" --tags "architecture,langchain"

# Search memories (semantic search)
mnemo search "LangChain" --memory-types "fact,skill" --limit 5

# Store code patterns
mnemo code-pattern "fastapi_basic" "from fastapi import FastAPI; app = FastAPI()" "python" "Basic FastAPI setup"

# Get workspace context
mnemo workspace-context "/my/project"

# Get statistics
mnemo stats
```

## ğŸ”§ Memory Types

- **Facts**: Static information and knowledge
- **Skills**: Learned patterns and techniques  
- **Preferences**: User preferences and settings
- **Conversations**: Chat history and context
- **Context**: Contextual information
- **Code Patterns**: Reusable code snippets and templates
- **Workspace**: Workspace-specific data
- **Project**: Project-specific information

## ğŸ¯ Memory Scopes

- **Global**: Available everywhere
- **Workspace**: Available in current workspace
- **Project**: Available in current project
- **Session**: Available in current session
- **Temporary**: Auto-expires

## ğŸ¤– Embedding Models

Mnemo supports multiple embedding models for different use cases:

### Recommended Models
- **Qwen3-Embedding-0.6B** â­: Best for M3/M2 Macs, lightweight (2GB), 100+ languages
- **intfloat/multilingual-e5-large**: High quality, 1024 dims, 3GB memory
- **paraphrase-multilingual-mpnet-base-v2**: Balanced performance, 768 dims

### Configuration
```python
# Use Qwen3 (default, lightweight)
from mnemo.core.embeddings import MnemoEmbeddings
embeddings = MnemoEmbeddings()  # Uses Qwen3-0.6B by default

# Use E5-Large (higher quality)
embeddings = MnemoEmbeddings(
    sentence_transformer_model="intfloat/multilingual-e5-large"
)

# Use OpenAI (requires API key)
embeddings = MnemoEmbeddings(
    use_mock=False,  # Force OpenAI if available
    openai_api_key="your-key"
)
```

## ğŸ”Œ MCP Integration (Cursor)

### Quick Setup ğŸš€
1. Start the MCP server:
```bash
# Run in a terminal
python -m mnemo.mcp.cli serve-fastapi
# or
python -m mnemo.mcp.fastapi_server
```

2. Add to `.cursor/mcp.json`:
```json
{
  "mcpServers": {
    "mnemo": {
      "url": "http://localhost:3333/mcp"
    }
  }
}
```

3. Restart Cursor and use:
```
@mnemo remember "project_info" "This is a FastAPI project with PostgreSQL"
@mnemo recall "project info"
```

ğŸ“š **Detailed guides for beginners:**
- [í•œêµ­ì–´ ê°€ì´ë“œ](docs/CURSOR_MCP_GUIDE_KR.md) - Cursor ì´ˆë³´ìë¥¼ ìœ„í•œ ì™„ë²½ ê°€ì´ë“œ
- [English Guide](docs/CURSOR_MCP_GUIDE_EN.md) - Complete guide for Cursor beginners

## ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ LangChain Tools â”‚  â† Agents & workflows
â”‚ Memory Chains   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Memory Client   â”‚  â† High-level API
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Vector Store    â”‚  â† LangChain integration
â”‚ (ChromaDB)      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Embeddings      â”‚  â† Semantic search
â”‚ (OpenAI/Custom) â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Optional:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ LangGraph       â”‚  â† Advanced workflows
â”‚ Workflows       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ”— LangChain Integration

### Memory Chains

```python
from langchain_openai import ChatOpenAI
from mnemo.chains.memory import MemoryChain

llm = ChatOpenAI()
memory_chain = MemoryChain(llm=llm, memory_client=client)

result = memory_chain({"query": "How do I use LangChain?"})
print(result["response"])  # Enhanced with relevant memories
```

### RAG Chains

```python
from mnemo.chains.rag import MemoryRAGChain

rag_chain = MemoryRAGChain(llm=llm, memory_client=client)
result = rag_chain({"question": "Show me FastAPI examples"})
```

### Memory Tools for Agents

```python
from langchain.agents import initialize_agent, AgentType
from mnemo.tools.memory import MemorySearchTool, MemoryStoreTool

tools = [
    MemorySearchTool(memory_client=client),
    MemoryStoreTool(memory_client=client)
]

agent = initialize_agent(tools, llm, agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION)
response = agent.run("Search for Python examples and store any new patterns you find")
```

### LangGraph Workflows (Optional)

```python
from mnemo.graph.memory_graph import MemoryWorkflowGraph

workflow = MemoryWorkflowGraph(llm=llm, memory_client=client)
result = workflow.run(
    query="Help me understand LangChain",
    should_learn=True  # Learn from this conversation
)
```

## ğŸ“ Project Structure

```
mnemo/
â”œâ”€â”€ core/            # Core types and utilities
â”‚   â”œâ”€â”€ types.py     # Memory types and models
â”‚   â””â”€â”€ embeddings.py # Custom embeddings
â”œâ”€â”€ memory/          # Memory system
â”‚   â”œâ”€â”€ store.py     # Vector store wrapper
â”‚   â””â”€â”€ client.py    # High-level client
â”œâ”€â”€ chains/          # LangChain chains
â”‚   â”œâ”€â”€ memory.py    # Memory-enhanced chains
â”‚   â””â”€â”€ rag.py       # RAG implementations
â”œâ”€â”€ tools/           # LangChain tools
â”‚   â””â”€â”€ memory.py    # Memory tools for agents
â”œâ”€â”€ graph/           # LangGraph workflows (optional)
â”‚   â””â”€â”€ memory_graph.py
â”œâ”€â”€ cli.py           # Command line interface
â””â”€â”€ examples/        # Usage examples
```

## ğŸ® Examples

Check out the `examples/` directory for detailed usage examples:

- `basic_langchain.py`: Basic LangChain-based memory operations
- `langchain_chains.py`: Memory chains and RAG examples  
- `langchain_tools.py`: Tools and agents with memory
- `langgraph_workflow.py`: Advanced LangGraph workflows (optional)

## ğŸ—ºï¸ Roadmap

### Phase 1: MCP Integration ğŸ”Œ
- [ ] MCP server implementation
- [ ] Cursor memory assistance integration
- [ ] Real-time memory sync
- [ ] Context injection for Cursor

### Phase 2: Multi-Vector DB Support ğŸ—„ï¸
- [x] ChromaDB (Current)
- [ ] Qdrant integration
- [ ] Pinecone support
- [ ] Weaviate adapter
- [ ] Abstract vector store interface

### Phase 3: Knowledge Graph Support ğŸ•¸ï¸
- [ ] RDF/SPARQL support
- [ ] Neo4j integration
- [ ] Graph-based memory relationships
- [ ] Hybrid vector + graph search

### Phase 4: Advanced Features ğŸš€
- [ ] Multi-modal memories (images, audio)
- [ ] Distributed memory sync
- [ ] Memory compression & optimization
- [ ] Fine-tuned embeddings for specific domains

## ğŸ”§ Development

### Requirements
- Python 3.11+
- ChromaDB (default vector store)
- OpenAI API key (optional, for embeddings)

### Testing
```bash
# Run tests
pytest

# Run with coverage
pytest --cov=mnemo
```

## ğŸ¤ Contributing

1. Fork the repository
2. Create a feature branch
3. Make your changes
4. Add tests
5. Submit a pull request

## ğŸ“„ License

Apache License 2.0 - see LICENSE file for details.

---

Built with â¤ï¸ for the AI assistant community